{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant calling with bcftools or custom function\n",
    "\n",
    "- Bcftools creates pileup from bam file, which in our case was generated using minimap2. To the variant calling, custom thresholds can be set.\n",
    "\n",
    "### Goal of the analysis\n",
    "\n",
    "- Identify variants based on quality (e.g Phred quality score) & Alignment frequency\n",
    "- If there are multiple variants, check which occur together and which are seperated (mixed population).\n",
    "- Check if the variant is an actual variant or sequencing error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pysam \n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from uncertainties import ufloat\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import combinations\n",
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/emre/github_repo/MinION\")\n",
    "\n",
    "from minION import analyser, consensus, variantcaller\n",
    "from minION.util import IO_processor\n",
    "importlib.reload(IO_processor)\n",
    "importlib.reload(analyser)\n",
    "importlib.reload(consensus)\n",
    "importlib.reload(variantcaller)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def call_and_filter_vcf(input_path, reference, allele_frequency):\n",
    "    \"\"\" Uses bcftools to call variants and filter them based on allele frequency.\n",
    "    Args:\n",
    "        - input_path (str): Path to the input files.\n",
    "        - reference (str): Path to the reference genome.\n",
    "        - allele_frequency (float): Allele frequency threshold.\n",
    "    Returns:\n",
    "        - Subprocess run\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    prompt_pileup = f\"bcftools mpileup -d 4000 -Ou -f {reference}  {input_path}/alignment.bam > {input_path}/pileup.bcf\"\n",
    "\n",
    "    prompt_call = f\"bcftools call -mv -Ob -o {input_path}/raw_variants.bcf {input_path}/pileup.bcf\"\n",
    "\n",
    "    prompt_view = f\"bcftools view -i 'INFO/AF>{allele_frequency}' -Ob -o {input_path}/filtered_variants.vcf {input_path}/raw_variants.bcf\"\n",
    "\n",
    "\n",
    "    subprocess.run(prompt_pileup, shell=True)\n",
    "\n",
    "    subprocess.run(prompt_call, shell=True)\n",
    "\n",
    "    subprocess.run(prompt_view, shell=True)\n",
    "\n",
    "    print(f\"Variant calling and filtering completed. Output saved to {input_path}/raw_variants_python.bcf\")\n",
    "\n",
    "\n",
    "def extract_positions_from_vcf(vcf_file : str) -> list:\n",
    "    \"\"\" Extracts the positions of the variants from a VCF file.\n",
    "    Args:\n",
    "        - vcf_file (str): Path to the VCF file. \n",
    "    Returns:\n",
    "        - positions (list): List of variant positions.\n",
    "    \"\"\"\n",
    "    positions = []\n",
    " \n",
    "    vcf = pysam.VariantFile(vcf_file)\n",
    "\n",
    "    for record in vcf:\n",
    "        positions.append(record.pos)\n",
    "    \n",
    "    vcf.close()\n",
    "\n",
    "    return positions\n",
    "\n",
    "def extract_mutations_from_vcf(vcf_file):\n",
    "    \"\"\" Extracts the mutations from a VCF file.\n",
    "    Args:\n",
    "        - vcf_file (str): Path to the VCF file.\n",
    "    Returns:\n",
    "        - formatted_mutations (str): Formatted mutations (e.g A100T_G223C).\n",
    "    \"\"\"\n",
    "\n",
    "    mutations = []\n",
    "\n",
    "    vcf = pysam.VariantFile(vcf_file)\n",
    "\n",
    "    for record in vcf:\n",
    "\n",
    "        ref_allele = record.ref\n",
    "        alt_alleles = record.alts\n",
    "\n",
    "        for alt_allele in alt_alleles:\n",
    "            position = record.pos\n",
    "            mutation = f\"{ref_allele}{position}{alt_allele}\"\n",
    "            mutations.append(mutation)\n",
    "\n",
    "    vcf.close()\n",
    "\n",
    "    formatted_mutations = \"_\".join(mutations)\n",
    "\n",
    "    return formatted_mutations\n",
    "\n",
    "\n",
    "def get_base_counts_at_position(bam_file, chrom, position):\n",
    "    \"\"\"\n",
    "    Extract unique bases and gaps (deletions) and their counts from a BAM file \n",
    "    at a specific position using pileup.\n",
    "    \n",
    "    Args:\n",
    "    - bam_file (str): path to the BAM file.\n",
    "    - chrom (str): chromosome or contig name.\n",
    "    - position (int): 1-based position to extract bases from.\n",
    "\n",
    "    Returns:\n",
    "    - dict: unique bases and gaps with their counts at the specified position.\n",
    "    \"\"\"\n",
    "    \n",
    "    bases = []\n",
    "\n",
    "    \n",
    "    with pysam.AlignmentFile(bam_file, 'rb') as bam:\n",
    "        for pileup_column in bam.pileup(chrom, position - 1, position, \n",
    "                                        min_base_quality=0, \n",
    "                                        min_mapping_quality=0, \n",
    "                                        truncate=True):\n",
    "            if pileup_column.pos == position - 1: \n",
    "                for pileup_read in pileup_column.pileups:\n",
    "                    if pileup_read.is_del:\n",
    "                        bases.append(\"-\")  \n",
    "                    elif not pileup_read.is_refskip:\n",
    "                        bases.append(pileup_read.alignment.query_sequence[pileup_read.query_position])\n",
    "\n",
    "    base_counts = Counter(bases)\n",
    "\n",
    "    return base_counts\n",
    "\n",
    "\n",
    "def generate_heatmap_data(bam_file, chrom, positions):\n",
    "    data = []\n",
    "    all_bases = set(['A', 'T', 'C', 'G', '-'])\n",
    "    for position in positions:\n",
    "        base_counts = get_base_counts_at_position(bam_file, chrom, position)\n",
    "        col = [base_counts.get(base, 0) for base in all_bases]\n",
    "        data.append(col)\n",
    "    df = pd.DataFrame(data, index=positions, columns=list(all_bases))\n",
    "    # Order as in all_bases\n",
    "    # TODO: Specify if a character if it is an actual gap or no coverage\n",
    "    df = df[['A', 'T', 'C', 'G', '-']]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_most_common_base(position, heatmap_data, ref_base):\n",
    "    non_ref_data = heatmap_data.loc[position].drop(ref_base)\n",
    "    return non_ref_data.idxmax(), non_ref_data.max()\n",
    "\n",
    "\n",
    "\n",
    "def get_bases_from_pileup(bam_file, chrom, positions):\n",
    "    bases_dict = {position: {} for position in positions}\n",
    "    \n",
    "    with pysam.AlignmentFile(bam_file, 'rb') as bam:\n",
    "        for pileup_column in bam.pileup(chrom, min(positions) - 1, max(positions) + 1,\n",
    "                                        min_base_quality=0, \n",
    "                                        min_mapping_quality=0, \n",
    "                                        truncate=True):\n",
    "            if pileup_column.pos + 1 in positions:\n",
    "                for pileup_read in pileup_column.pileups:\n",
    "                    if not pileup_read.is_del and not pileup_read.is_refskip:\n",
    "                        base = pileup_read.alignment.query_sequence[pileup_read.query_position]\n",
    "                        read_name = pileup_read.alignment.query_name\n",
    "                        # Check if the read name is already added to the bases_dict for that position\n",
    "                        if read_name not in bases_dict[pileup_column.pos + 1]:\n",
    "                            bases_dict[pileup_column.pos + 1][read_name] = base\n",
    "\n",
    "    # Get unique read names and sort them to have a consistent order\n",
    "    read_names = sorted(set().union(*[bases_dict[pos].keys() for pos in bases_dict]))\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(index=read_names, columns=positions)\n",
    "    \n",
    "    # Populate DataFrame\n",
    "    for pos in positions:\n",
    "        for read_name, base in bases_dict[pos].items():\n",
    "            df.at[read_name, pos] = base\n",
    "    \n",
    "    # Fill NaN with \"-\"\n",
    "    df = df.fillna(\"-\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_max_base_at_position(bam_file, chrom, position):\n",
    "    base_counts = get_base_counts_at_position(bam_file, chrom, position)\n",
    "    return base_counts.most_common(1)[0][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "barcodes = IO_processor.get_barcode_dict(Path(\"/home/emre/minION_results/MinION_RBC_0902723_sup/Demultiplex_cpp_70_short\"))\n",
    "#path_to_bam = \"/home/emre/minION_results/MinION_RBC_0902723_sup/Demultiplex_cpp_70_short/RB03/NB08/consensus\"\n",
    "path_to_bam = \"/home/emre/minION_results/20231119_1502_MN41105/Demultiplex_cpp_70/RB01/NB22/consensus\"\n",
    "#template_fasta = \"/home/emre/github_repo/MinION/minION/refseq/hetcpiii.fasta\"\n",
    "#template_fasta = Path(\"/home/emre/tam-lqv.fasta\")\n",
    "template_fasta = Path(\"/home/emre/PgA9.fasta\")\n",
    "#output_vcf = f\"{path_to_bam}/raw_variants.vcf\"\n",
    "#consensus.get_consensus(Path(path_to_bam), Path(template_fasta))\n",
    "#call_and_filter_vcf(path_to_bam, template_fasta, 0.4)\n",
    "bam_file = f'{path_to_bam}/alignment.bam'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# Initialize Variant Caller\n",
    "importlib.reload(variantcaller)\n",
    "experiment_folder = Path(\"/home/emre/minION_results/MinION_RBC_0902723_sup\")\n",
    "template_fasta = \"/home/emre/github_repo/MinION/data/refseq/hetcpiii_padded.fasta\"\n",
    "demultiplex_folder_name = \"Demultiplex_cpp_70_200k_reads\"\n",
    "vc = variantcaller.VariantCaller(experiment_folder, template_fasta, demultiplex_folder_name=demultiplex_folder_name, padding_start=50, padding_end=50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "bam_file = Path(\"/home/emre/minION_results/MinION_RBC_0902723_sup/Demultiplex_cpp_70_200k_reads/RB01/NB69/alignment_minimap.bam\")\n",
    "nb_positions = vc._get_postion_range(padding_start=50, padding_end=50)\n",
    "freq_dist = vc._get_highest_non_ref_base_freq(bam_file, nb_positions, threshold=0.3)[0]\n",
    "nb_positions = vc._get_nb_positions(freq_dist)\n",
    "if not nb_positions:\n",
    "    print(\"No non-reference bases found.\")\n",
    "\n",
    "nb_positions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "vc.variant_df.iloc[65:75,:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "variant = vc.get_variant_df(nb_positions)\n",
    "variant"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "variant.tail(20)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "source": [
    "bases_df, qual_df = vc._get_bases_from_pileup(bam_file, nb_positions)\n",
    "\n",
    "bases_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "source": [
    "nb_positions = [642]\n",
    "softmax_df = vc._get_softmax_count_df(bases_df, qual_df, nb_positions)\n",
    "\n",
    "softmax_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "source": [
    "nb_positions = [157]\n",
    "alphabet = \"ACTG-\"\n",
    "\n",
    "softmax_counts = {position: [] for position in nb_positions}\n",
    "\n",
    "for position in nb_positions:\n",
    "    for base in alphabet:\n",
    "        base_mask = (bases_df[position] == base)\n",
    "        base_counts = base_mask.sum()\n",
    "        # Calculate the non-error probability for each base and sum them up\n",
    "\n",
    "        soft_count = sum(base_mask * qual_df[position].apply(vc._get_non_error_prop))\n",
    "\n",
    "        qual_sum = qual_df[position].sum()\n",
    "        print(f\"Position: {position}, Base: {base}, Soft count: {soft_count}, Base count: {base_counts}, Total qual: {qual_sum}\")\n",
    "        softmax_counts[position].append(soft_count)\n",
    "\n",
    "softmax_count_df = pd.DataFrame(softmax_counts, columns=nb_positions, index=list(alphabet))\n",
    "\n",
    "# Apply softmax to each column (position)\n",
    "softmax_count_df = softmax_count_df.apply(lambda x: x / x.sum(), axis=0)\n",
    "\n",
    "softmax_count_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "source": [
    "qual_df.iloc[210:215,:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "source": [
    "qual_df[642].shape == base_mask.shape\n",
    "\n",
    "soft_ = base_mask * qual_df[642].apply(vc._get_non_error_prop)\n",
    "softmax_counts[position].append(soft_count)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "source": [
    "sum(soft_.head(212))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "source": [
    "soft_[210:213]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "soft_df = vc._get_softmax_count_df(bases_df, qual_df, nb_positions)\n",
    "\n",
    "vc._call_potential_populations(soft_df, call_threshold=0.1)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "variants = vc.call_variant(bam_file, threshold=0.3)\n",
    "np.array(variants).argmax()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "nb_positions = extract_positions_from_vcf(f'{path_to_bam}/filtered_variants.vcf')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#reference = \"HetCPIII\"\n",
    "#reference = \"Tam-LQV\"\n",
    "reference = \"PgA9\"  \n",
    "position_to_check = 1  \n",
    "bases_at_position = get_base_counts_at_position(bam_file, reference, position_to_check)\n",
    "bases_at_position"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#heatmap_data = generate_heatmap_data(bam_file, reference, 1, 100)\n",
    "nb_positions = [23,38,56,98,421]\n",
    "heatmap_data = generate_heatmap_data(bam_file, reference, nb_positions)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "heatmap_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "\n",
    "template = analyser.get_template_sequence(template_fasta)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.heatmap(heatmap_data, \n",
    "            annot=False, \n",
    "            cmap='YlGnBu', \n",
    "            cbar_kws={'label': 'Count'}, \n",
    "            linewidths=0.5, \n",
    "            linecolor='white')\n",
    "plt.title(\"Base Counts per Position\")\n",
    "plt.xlabel(\"Bases\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "bam_file"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Highest Non Reference Base Frequency\n",
    "\n",
    "- Bam file\n",
    "- Reference sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "ref_seq = analyser.get_template_sequence(template_fasta)\n",
    "freq_dist = pd.DataFrame(analyser.get_highest_non_ref_base_freq(bam_file, reference, range(1,len(ref_seq)), ref_seq)[0]).T"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "freq_dist"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Replace NaN value with \"T\"\n",
    "freq_dist[0] = freq_dist[0].fillna(\"None\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "colors_for_bases = {\"-\" : \"grey\", \"A\" : \"red\", \"T\" : \"blue\", \"C\" : \"green\", \"G\" : \"yellow\", \"None\" : \"black\"}\n",
    "\n",
    "freq_dist = freq_dist.rename(columns={0:\"Base\", 1:\"Frequency\"})\n",
    "\n",
    "freq_dist[\"Base\"] = freq_dist[\"Base\"].fillna(\"None\")\n",
    "\n",
    "freq_dist = freq_dist.dropna()\n",
    "\n",
    "freq_dist[\"Color\"] = freq_dist[\"Base\"].map(colors_for_bases)\n",
    "\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x=freq_dist.index, y=freq_dist[\"Frequency\"], palette=freq_dist[\"Color\"])\n",
    "#plt.title(\"Highest Non-Reference Base Frequency \")\n",
    "plt.xticks([])\n",
    "plt.xlabel(\"Positions\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# TODO Add horizontal line at 2 mean std deviation\n",
    "plt.axhline(y=0.3, color='b', linestyle='--')\n",
    "# TODO Add legend for bases\n",
    "nb_positions = []\n",
    "for index, row in freq_dist.iterrows():\n",
    "    if row[\"Frequency\"] > 0.3:\n",
    "        nb_positions.append(index)\n",
    "        plt.text(index, row[\"Frequency\"], f\"{index}{row['Base']}\", color='black', ha=\"center\", va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig(\"/home/emre/github_repo/MinION/examples/figures/PgA9_variant_freq_RB02_NB92.png\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_nb_positions(freq_dist, threshold):\n",
    "    nb_positions = []\n",
    "    for index, row in freq_dist.iterrows():\n",
    "        if row[\"Frequency\"] > threshold:\n",
    "            nb_positions.append(index)\n",
    "    return nb_positions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "bases_df = get_bases_from_pileup(bam_file, reference, nb_positions)\n",
    "\n",
    "heatmap_data = generate_heatmap_data(bam_file, reference, nb_positions)\n",
    "pval_matrix = np.zeros((len(nb_positions), len(nb_positions)))\n",
    "odds_ratio_matrix = np.zeros((len(nb_positions), len(nb_positions)))\n",
    "jaccard_index_matrix = np.zeros((len(nb_positions), len(nb_positions)))\n",
    "overlap_matrix = np.zeros((len(nb_positions), len(nb_positions)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate combinations of positions and initialize storage structures\n",
    "variant = []\n",
    "if nb_positions is not None:\n",
    "    heatmap_data = generate_heatmap_data(bam_file, reference, nb_positions)\n",
    "    position_combinations = list(combinations(nb_positions, 2))\n",
    "    jaccard_indices = {\"Pair\": [],\"Jaccard Index\": [],}\n",
    "    bases_df = get_bases_from_pileup(bam_file, reference, nb_positions)\n",
    "\n",
    "    if len(nb_positions) == 1:\n",
    "\n",
    "        variant = f\"{template[nb_positions[0] - 1]}{nb_positions[0]}{get_most_common_base([0], heatmap_data, template[nb_positions[0] - 1])[0]}\"\n",
    "\n",
    "\n",
    "    elif len(nb_positions) > 1:\n",
    "        # Process each pair of positions\n",
    "        for pos1, pos2 in position_combinations:\n",
    "            common_base1, common_base2 = [get_most_common_base(pos, heatmap_data, template[pos - 1])[0] for pos in [pos1, pos2]]\n",
    "            ref_bases = [template[pos - 1] for pos in [pos1, pos2]]\n",
    "\n",
    "            counts = {}\n",
    "            for idx, (base1, base2) in enumerate([(common_base1, common_base2), (ref_bases[0], common_base2), (common_base1, ref_bases[1]), (ref_bases[0], ref_bases[1])]):\n",
    "                counts[idx] = len(bases_df[(bases_df[pos1] == base1) & (bases_df[pos2] == base2)]) + 1\n",
    "\n",
    "            # Calculate Jaccard indices\n",
    "            jaccard_dict = {f\"A_{i}_and_B_{j}\": counts[k] / (counts[k] + counts[(k + 1) % 4] + counts[(k + 2) % 4]) for k, (i, j) in enumerate([('mut', 'mut'), ('wt', 'mut'), ('mut', 'wt'), ('wt', 'wt')])}\n",
    "\n",
    "            jaccard_indices[\"Pair\"].append((pos1, pos2))\n",
    "            jaccard_indices[\"Jaccard Index\"].append(jaccard_dict)\n",
    "\n",
    "            jaccard_index_matrix[nb_positions.index(pos1), nb_positions.index(pos2)] = jaccard_dict[\"A_mut_and_B_mut\"]\n",
    "\n",
    "            contingency_table = pd.DataFrame({\n",
    "                \"mut_A\": [jaccard_dict[\"A_mut_and_B_mut\"], jaccard_dict[\"A_mut_and_B_wt\"]],\n",
    "                \"wt_A\": [jaccard_dict[\"A_wt_and_B_mut\"], jaccard_dict[\"A_wt_and_B_wt\"]],\n",
    "            }, index=['mut_B', 'wt_B'])\n",
    "\n",
    "        mutation_details = {\"Type\": [], \"Position\": [], \"Variant\": [], \"Frequency\": []}\n",
    "        for i, (pair, jaccard_dict) in enumerate(zip(jaccard_indices[\"Pair\"], jaccard_indices[\"Jaccard Index\"])):\n",
    "            max_type = max(jaccard_dict, key=jaccard_dict.get)\n",
    "            if jaccard_dict[max(jaccard_dict, key=jaccard_dict.get)] > 0.7 and max_type != \"A_wt_and_B_wt\":\n",
    "\n",
    "                ref_bases = [template[pos - 1] for pos in pair]\n",
    "                common_base1, common_base2 = [get_most_common_base(pos, heatmap_data, template[pos - 1])[0] for pos in pair]\n",
    "                if max_type == \"A_mut_and_B_mut\":\n",
    "                    mutation_details[\"Type\"].extend([\"Co-Occuring\"])\n",
    "                    mutation_details[\"Position\"].extend(pair)\n",
    "                    mutation_details[\"Variant\"].extend([f'{ref_bases[0]}{pair[0]}{common_base1}', f'{ref_bases[1]}{pair[1]}{common_base2}'])\n",
    "                    mutation_details[\"Frequency\"].extend([jaccard_dict[\"A_mut_and_B_mut\"], jaccard_dict[\"A_mut_and_B_mut\"]])\n",
    "                elif max_type == \"A_mut_and_B_wt\":\n",
    "                    mutation_details[\"Type\"].extend([\"A\"])\n",
    "                    mutation_details[\"Position\"].append(pair[0])\n",
    "                    mutation_details[\"Variant\"].append(f'{ref_bases[0]}{pair[0]}{common_base1}')\n",
    "                    mutation_details[\"Frequency\"].append(jaccard_dict[\"A_mut_and_B_wt\"])\n",
    "                elif max_type == \"A_wt_and_B_mut\":\n",
    "                    mutation_details[\"Type\"].extend([\"B\"])\n",
    "                    mutation_details[\"Position\"].append(pair[1])\n",
    "                    mutation_details[\"Variant\"].append(f'{ref_bases[1]}{pair[1]}{common_base2}')\n",
    "                    mutation_details[\"Frequency\"].append(jaccard_dict[\"A_wt_and_B_mut\"])\n",
    "            \n",
    "\n",
    "\n",
    "        # Create a DataFrame from the mutation details of Position and Variant only\n",
    "        mutation_details_df = pd.DataFrame(mutation_details, columns=[\"Position\", \"Variant\", \"Frequency\"])\n",
    "\n",
    "\n",
    "        mutation_details_df = mutation_details_df.sort_values(by=[\"Position\"])\n",
    "        mutation_details_df = mutation_details_df[\"Variant\"].drop_duplicates()\n",
    "\n",
    "        # TODO: Try to explain why we set up a threshold of 10\n",
    "        if len(mutation_details_df) > 10:\n",
    "            variant = \"NA\"\n",
    "        else:\n",
    "\n",
    "            variant = \"_\".join(list(mutation_details_df))\n",
    "\n",
    "        #Plot heatmap for Jaccard indices\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(jaccard_index_matrix, cmap='coolwarm', mask=np.tri(len(nb_positions), k=-1), linewidths=0.5, linecolor='white')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Position')\n",
    "        plt.title('Jaccard Index Heatmap')\n",
    "else:\n",
    "    variant = \"#PARENT#\"\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate the frequency of each unique row\n",
    "frequency_df = bases_df.apply(lambda row: '-'.join(row), axis=1).value_counts().reset_index()\n",
    "frequency_df.columns = ['Population', 'Frequency']\n",
    "frequency_df[\"Frequency\"] = frequency_df[\"Frequency\"] / frequency_df[\"Frequency\"].sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import tqdm\n",
    "\n",
    "def get_variant_name(entry, reference, nb_positions):\n",
    "    variant = []\n",
    "    for i, mut in enumerate(entry):\n",
    "        \n",
    "        ref_AA = reference[nb_positions[i] - 1]\n",
    "        \n",
    "        if mut == \"-\":\n",
    "            v = f'{ref_AA}{nb_positions[i]}DEL'\n",
    "        \n",
    "        elif mut == ref_AA:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            v = f'{ref_AA}{nb_positions[i]}{mut}'\n",
    "\n",
    "        variant.append(v)\n",
    "\n",
    "    return \"_\".join(variant)\n",
    "\n",
    "def get_nb_positions(freq_dist, threshold):\n",
    "    nb_positions = []\n",
    "    for index, row in freq_dist.iterrows():\n",
    "        if row[\"Frequency\"] > threshold:\n",
    "            nb_positions.append(index)\n",
    "    return nb_positions\n",
    "\n",
    "def get_pop_frequency(bam_file, template, reference, nb_positions, min_freq=0.1, min_depth= 15):\n",
    "\n",
    "    # Get PileUP\n",
    "    bases_df = get_bases_from_pileup(bam_file, reference, nb_positions)\n",
    "    \n",
    "    frequency_df = bases_df.apply(get_variant_name, axis=1, args=(template, nb_positions)).value_counts().reset_index()\n",
    "\n",
    "    frequency_df.columns = ['Population', 'N_reads']\n",
    "    \n",
    "    frequency_df[\"Frequency\"] = frequency_df[\"N_reads\"] / frequency_df[\"N_reads\"].sum()\n",
    "\n",
    "    # Filter for frequency > 0.1\n",
    "    frequency_df = frequency_df[(frequency_df[\"Frequency\"] > min_freq) & (frequency_df[\"N_reads\"] > min_depth)]\n",
    "\n",
    "    return frequency_df\n",
    "\n",
    "def call_variant_pop_frequency(bam_file, template, reference, min_freq=0.1, min_depth= 15):\n",
    "    \"\"\" \n",
    "    Call Variant from BAM file based on Basecall Frequency & Alignment Frequency\n",
    "\n",
    "    Args:\n",
    "        - bam_file (str): Path to the BAM file.\n",
    "        - template (str): Template name >NAME from fasta or fastq files.\n",
    "        - reference (str): Reference name >NAME from fasta or fastq files.\n",
    "        - nb_positions (list): List of positions to call variants.\n",
    "        - min_freq (float): Minimum frequency of the variant.\n",
    "        - min_depth (int): Minimum depth of the variant.\n",
    "    \n",
    "    Returns:\n",
    "        - variant (str): Variant name.\n",
    "    \"\"\"\n",
    "\n",
    "    variants = {\"Variant\" : [], \"Position\" : [], \"Alignment Count\" : [], \"Alignment Frequency\" : []}\n",
    "\n",
    "\n",
    "    try:\n",
    "        alignment_count = int(subprocess.run(f\"samtools view -c {bam_file}\", shell=True, capture_output=True).stdout.decode(\"utf-8\").strip())\n",
    "        freq_dist = pd.DataFrame(analyser.get_highest_non_ref_base_freq(bam_file, reference, range(1,len(template)), template)[0]).T.rename(columns={0:\"Base\", 1:\"Frequency\"})\n",
    "\n",
    "        nb_positions = get_nb_positions(freq_dist, 0.4)\n",
    "\n",
    "        if nb_positions == [] and alignment_count > min_depth:\n",
    "            variants[\"Variant\"].append(\"#PARENT#\")\n",
    "            variants[\"Position\"].append(\"-\")\n",
    "            variants[\"Alignment Count\"].append(alignment_count)\n",
    "            variants[\"Alignment Frequency\"].append(\"-\")\n",
    "            \n",
    "\n",
    "        elif nb_positions == [] and alignment_count < min_depth:\n",
    "            variants[\"Variant\"].append(\"NA\")\n",
    "            variants[\"Position\"].append(\"-\")\n",
    "            variants[\"Alignment Count\"].append(alignment_count)\n",
    "            variants[\"Alignment Frequency\"].append(\"-\")\n",
    "\n",
    "        elif len(nb_positions) == 1 and alignment_count > min_depth:\n",
    "\n",
    "            ref_base = template[nb_positions[0] - 1]\n",
    "            pos = nb_positions[0]\n",
    "            base = freq_dist[\"Base\"][nb_positions[0] - 1]\n",
    "\n",
    "            variants[\"Variant\"].append(f\"{ref_base}{pos}{base}\")\n",
    "            variants[\"Position\"].append(nb_positions[0])\n",
    "            variants[\"Alignment Count\"].append(alignment_count)\n",
    "            variants[\"Alignment Frequency\"].append(freq_dist[\"Frequency\"][nb_positions[0] - 1])\n",
    "\n",
    "           \n",
    "        else:\n",
    "\n",
    "            freq_df = get_pop_frequency(bam_file, template, reference, nb_positions, min_freq=0.1)\n",
    "\n",
    "            for index, row in freq_df.iterrows():\n",
    "                variant = row[\"Population\"]\n",
    "                variants[\"Variant\"].append(variant)\n",
    "                variants[\"Position\"].append(nb_positions)\n",
    "                variants[\"Alignment Count\"].append(row[\"N_reads\"])\n",
    "                variants[\"Alignment Frequency\"].append(row[\"Frequency\"])\n",
    "\n",
    "    except:\n",
    "        print(\"Error in calling variant\")\n",
    "        variants[\"Variant\"].append(\"NA\")\n",
    "        variants[\"Position\"].append(\"-\")\n",
    "\n",
    "    return variants\n",
    "\n",
    "def get_variant_df_AF(demultiplex_folder: Path, ref_seq : Path, ref_name : str, barcode_dicts : dict = None, consensus_folder_name = \"consensus\", merge = True, min_freq=0.1, min_depth= 15):\n",
    "\n",
    "    if barcode_dicts is None:\n",
    "        barcode_dicts = IO_processor.get_barcode_dict(demultiplex_folder)\n",
    "    \n",
    "    variant_template_df = analyser.template_df(barcode_dicts, rowwise=False)\n",
    "\n",
    "    variants = {\"RBC\": [], \"FBC\": [], \"Position\": [], \"Variant\": [], \"Alignment Count\": [], \"Alignment Frequency\": []}\n",
    "\n",
    "    template = analyser.get_template_sequence(ref_seq) # Reference sequence\n",
    "\n",
    "    summary = analyser.read_summary_file(demultiplex_folder)\n",
    "    n_counts = summary.groupby([\"RBC\",\"FBC\"])[\"FBC\"].value_counts().reset_index() \n",
    "\n",
    "\n",
    "\n",
    "    for barcode_id, barcode_dict in barcode_dicts.items():\n",
    "\n",
    "        rbc = os.path.basename(barcode_id)\n",
    "\n",
    "        for front_barcode in barcode_dict:\n",
    "\n",
    "            fbc = os.path.basename(front_barcode)\n",
    "\n",
    "            count = n_counts[ (n_counts[\"RBC\"] == rbc) & (n_counts[\"FBC\"] == fbc)][\"count\"].values[0]\n",
    "\n",
    "            bam_file = front_barcode / consensus_folder_name / \"alignment.bam\"\n",
    "\n",
    "            if not bam_file.exists():\n",
    "                print(f\"{bam_file} does not exist.\")\n",
    "                variants[\"RBC\"].append(rbc)\n",
    "                variants[\"FBC\"].append(fbc)\n",
    "                variants[\"Position\"].append([\"NA\"])\n",
    "                variants[\"Variant\"].append([\"NA\"])\n",
    "                #variants[\"Reads\"].append(count)\n",
    "                variants[\"Alignment Count\"].append([\"NA\"])\n",
    "                variants[\"Alignment Frequency\"].append([\"NA\"])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                \n",
    "                nn_variants = call_variant_pop_frequency(bam_file, template, ref_name, min_freq, min_depth) # TODO read header from reference file for name\n",
    "\n",
    "                for i, variant in enumerate(nn_variants[\"Variant\"]):\n",
    "                    variants[\"RBC\"].append(rbc)\n",
    "                    variants[\"FBC\"].append(fbc)\n",
    "                    variants[\"Position\"].append(nn_variants[\"Position\"][i])\n",
    "                    variants[\"Variant\"].append(variant)\n",
    "                    #variants[\"Reads\"].append(count)\n",
    "                    variants[\"Alignment Count\"].append(nn_variants[\"Alignment Count\"][i])\n",
    "                    variants[\"Alignment Frequency\"].append(nn_variants[\"Alignment Frequency\"][i])\n",
    "            \n",
    "            except:\n",
    "                print(f\"Skipping {rbc}/{fbc}\")\n",
    "                continue\n",
    "\n",
    "    variant_df = analyser.rename_barcode(pd.DataFrame(variants).merge(n_counts, on=[\"RBC\",\"FBC\"] , how=\"left\"))\n",
    "\n",
    "    if merge:\n",
    "        return variant_df.merge(variant_template_df, on=[\"Plate\", \"Well\"], how=\"right\")\n",
    "    else:\n",
    "        return variants\n",
    "\n",
    "\n",
    "\n",
    "def get_mixed_population(variant_df):\n",
    "\n",
    "    # Get row that are not unique\n",
    "    mixed_pop = variant_df[variant_df.duplicated(subset=[\"Plate\", \"Well\"], keep=False)]\n",
    "\n",
    "    return mixed_pop\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "path_to_bam = \"/home/emre/minION_results/20231119_1502_MN41105/Demultiplex_cpp_70/RB01/NB94/consensus\"\n",
    "bam_file = f'{path_to_bam}/alignment.bam'\n",
    "template = analyser.get_template_sequence(Path(\"/home/emre/PgA9.fasta\"))\n",
    "reference = \"PgA9\"\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "variant_df = get_variant_df_AF(Path(\"/home/emre/minION_results/20231119_1502_MN41105/Demultiplex_cpp_70\"), Path(\"/home/emre/PgA9.fasta\"), ref_name =  \"PgA9\", min_freq=0.1, min_depth= 15)\n",
    "\n",
    "\n",
    "r"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "variant_df.to_csv(\"data/PgA9_variant_df.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "variant_df = pd.read_csv(\"data/PgA9_variant_df.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "get_mixed_population(variant_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df = variant_df.dropna(subset=[\"Variant\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df[\"count\"].describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(len(variants[\"RBC\"]))\n",
    "print(len(variants[\"FBC\"]))\n",
    "#print(len(variants[\"Position\"]))\n",
    "print(len(variants[\"Variant\"]))\n",
    "print(len(variants[\"Alignment Count\"]))\n",
    "print(len(variants[\"Alignment Frequency\"]))\n",
    "\n",
    "test_df = pd.DataFrame({\"RBC\": variants[\"RBC\"], \"FBC\": variants[\"FBC\"], \"Variant\": variants[\"Variant\"], \"Alignment Count\": variants[\"Alignment Count\"], \"Alignment Frequency\": variants[\"Alignment Frequency\"]})\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "test_df[test_df[\"RBC\"] == \"RB01\"].sort_values(by=[\"FBC\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "barcode_dicts = IO_processor.get_barcode_dict(Path(\"/home/emre/minION_results/TamLQV96_sup/Demultiplex_cpp_70\"))\n",
    "demultiplex_folder = Path(\"/home/emre/minION_results/TamLQV96_sup/Demultiplex_cpp_70\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "bases, qualities = analyser.get_highest_non_ref_base_freq(bam_file, reference, range(1,len(ref_seq)), ref_seq)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "variant_df = analyser.get_variant_df_custom(demultiplex_folder, template_fasta, barcode_dicts, consensus_folder_name = \"consensus\", merge = True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "variant_df.dropna(subset=[\"count\"], inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def single_plate_annotation(entry):\n",
    "    row = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "    new_well_name = row[int(entry[\"Plate\"]) - 1] + entry[\"Well\"][1:]\n",
    "    entry[\"Well\"] = new_well_name\n",
    "    return entry"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df = variant_df.apply(single_plate_annotation, axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Define rows and columns for the 96-well plate\n",
    "rows = list('ABCDEFGH')\n",
    "columns = range(1, 13)\n",
    "\n",
    "# Generate well identifiers\n",
    "wells = [f\"{row}{col:01d}\" for row in rows for col in columns]\n",
    "\n",
    "# # Generate random reads data\n",
    "# np.random.seed(0)  # For reproducible results\n",
    "# reads = np.random.randint(50, 1000, size=len(wells))  # Random reads between 50 and 1000\n",
    "\n",
    "# # Create the DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'Well': wells,\n",
    "#     'Reads': reads\n",
    "# })\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "variant_df.to_csv(\"data/Tam_LQV_variant_df.csv\", index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "# Initialize the data matrix with NaN for the heatmap\n",
    "heatmap_data = np.full((8, 12), np.nan)\n",
    "\n",
    "# Fill in the data matrix with 'Reads' values\n",
    "for _, row in df.iterrows():\n",
    "    well_row = 'ABCDEFGH'.index(row['Well'][0])\n",
    "    well_col = int(row['Well'][1:]) - 1\n",
    "    heatmap_data[well_row, well_col] = row['count']\n",
    "\n",
    "# Labels\n",
    "col_labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "row_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "\n",
    "# Heatmap\n",
    "im = ax.imshow(heatmap_data, cmap=\"Blues\", aspect='auto')\n",
    "\n",
    "# Colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel(\"# of Reads\", rotation=90, va=\"top\", size=14)\n",
    "\n",
    "# White grid\n",
    "ax.set_xticks(np.arange(-.5, 12, 1), minor=True)\n",
    "ax.set_yticks(np.arange(-.5, 8, 1), minor=True)\n",
    "ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=2)\n",
    "ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "# Annotations with variants\n",
    "for (i, j), val in np.ndenumerate(heatmap_data):\n",
    "    if not np.isnan(val):  # Only annotate non-NaN values\n",
    "        variant_text = '\\n'.join(df.loc[df['Well'] == f'{row_labels[i]}{j+1:01d}', 'Variant'].values[0])\n",
    "        ax.text(j, i, variant_text, ha='center', va='center', color='red', fontsize=8)\n",
    "    elif np.isnan(val):\n",
    "        ax.text(j, i, \"NA\", ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "\n",
    "\n",
    "# Set the ticks\n",
    "ax.set_xticks(np.arange(len(col_labels)))\n",
    "ax.set_yticks(np.arange(len(row_labels)))\n",
    "ax.set_xticklabels(col_labels)\n",
    "ax.set_yticklabels(row_labels)\n",
    "\n",
    "# Turn off the tick labels if not needed\n",
    "ax.tick_params(axis=u'both', which=u'both',length=0)\n",
    "\n",
    "plt.savefig(\"figures/heatmap_tam.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "variant_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Backup\n",
    "\n",
    "# Calculate for each pair chi-squared test\n",
    "#nb_positions = extract_positions_from_vcf(f'{path_to_bam}/filtered_variants.vcf')\n",
    "comb = list(combinations(nb_positions, 2))\n",
    "\n",
    "\n",
    "# Initialize a matrix with zeros for storing the -log10(p-values)\n",
    "jaccard_index_meta = {\"Pair\" : [], \"Jaccard Index Dict\" : []}\n",
    "\n",
    "bases_df = get_bases_from_pileup(bam_file, reference, nb_positions)\n",
    "\n",
    "for pair in comb:\n",
    "    \n",
    "    max_nucleotide_pair1, _ = get_most_common_base(pair[0], heatmap_data, template[pair[0] - 1])\n",
    "    max_nucleotide_pair2, _ = get_most_common_base(pair[1], heatmap_data, template[pair[1] - 1])\n",
    "\n",
    "    ref_nuc_pair1 = template[pair[0] - 1]\n",
    "    ref_nuc_pair2 = template[pair[1] - 1]\n",
    "\n",
    "\n",
    "    # Mutations\n",
    "    A_and_B = len(bases_df[(bases_df[pair[0]] == max_nucleotide_pair1) & (bases_df[pair[1]] == max_nucleotide_pair2)])+ 1\n",
    "    A_wo_B = len(bases_df[(bases_df[pair[0]] == max_nucleotide_pair1) & (bases_df[pair[1]] != max_nucleotide_pair2)]) + 1\n",
    "    B_wo_A = len(bases_df[(bases_df[pair[0]] != max_nucleotide_pair1) & (bases_df[pair[1]] == max_nucleotide_pair2)]) + 1\n",
    "    wo_A_and_B = len(bases_df[(bases_df[pair[0]] != max_nucleotide_pair1) & (bases_df[pair[1]] != max_nucleotide_pair2)]) + 1\n",
    "\n",
    "\n",
    "    # A wt vs Mutations\n",
    "    A_wt_and_B = len(bases_df[(bases_df[pair[0]] == ref_nuc_pair1) & (bases_df[pair[1]] == max_nucleotide_pair2)]) + 1\n",
    "    A_wt_wo_B = len(bases_df[(bases_df[pair[0]] == ref_nuc_pair1) & (bases_df[pair[1]] != max_nucleotide_pair2)]) + 1\n",
    "    B_wo_A_wt = len(bases_df[(bases_df[pair[0]] != ref_nuc_pair1) & (bases_df[pair[1]] == max_nucleotide_pair2)]) + 1\n",
    "    wo_A_wt_and_B = len(bases_df[(bases_df[pair[0]] != ref_nuc_pair1) & (bases_df[pair[1]] != max_nucleotide_pair2)]) + 1\n",
    "\n",
    "    # B wt vs Mutations\n",
    "    A_and_B_wt  = len(bases_df[(bases_df[pair[0]] == max_nucleotide_pair1) & (bases_df[pair[1]] == ref_nuc_pair2)]) + 1\n",
    "    A_wo_B_wt = len(bases_df[(bases_df[pair[0]] == max_nucleotide_pair1) & (bases_df[pair[1]] != ref_nuc_pair2)]) + 1\n",
    "    B_wo_A_wt = len(bases_df[(bases_df[pair[0]] != max_nucleotide_pair1) & (bases_df[pair[1]] == ref_nuc_pair2)]) + 1\n",
    "    wo_A_and_B_wt = len(bases_df[(bases_df[pair[0]] != max_nucleotide_pair1) & (bases_df[pair[1]] != ref_nuc_pair2)]) + 1\n",
    "\n",
    "    # A wt vs B wt\n",
    "    A_wt_and_B_wt = len(bases_df[(bases_df[pair[0]] == ref_nuc_pair1) & (bases_df[pair[1]] == ref_nuc_pair2)]) + 1\n",
    "    A_wt_wo_B_wt = len(bases_df[(bases_df[pair[0]] == ref_nuc_pair1) & (bases_df[pair[1]] != ref_nuc_pair2)]) + 1\n",
    "    B_wo_A_wt_wt = len(bases_df[(bases_df[pair[0]] != ref_nuc_pair1) & (bases_df[pair[1]] == ref_nuc_pair2)]) + 1\n",
    "    wo_A_wt_and_B_wt = len(bases_df[(bases_df[pair[0]] != ref_nuc_pair1) & (bases_df[pair[1]] != ref_nuc_pair2)]) + 1\n",
    "\n",
    "\n",
    "    # Jaccardi index dictionary\n",
    "    print(\"Pair:\", pair)\n",
    "    jaccard_index_dict = {  \"A_and_B\": A_and_B / (A_and_B + A_wo_B + B_wo_A),\n",
    "                            \"A_and_B_wt\": A_and_B_wt / (A_and_B_wt + A_wo_B_wt + B_wo_A_wt),\n",
    "                            \"A_wt_and_B\": A_wt_and_B / (A_wt_and_B + A_wt_wo_B + B_wo_A_wt),\n",
    "                            \"A_wt_and_B_wt\": A_wt_and_B_wt / (A_wt_and_B_wt + A_wt_wo_B_wt + B_wo_A_wt_wt)}\n",
    "\n",
    "    jaccard_index_meta[\"Pair\"].append(pair)\n",
    "    jaccard_index_meta[\"Jaccard Index Dict\"].append(jaccard_index_dict)\n",
    "\n",
    "    print(jaccard_index_dict)\n",
    "\n",
    "\n",
    "    #print(\"Jaccard index:\", jaccard_index)\n",
    "\n",
    "    # contingency_table = pd.DataFrame({\n",
    "    #     'B': [A_and_B, A_wo_B],\n",
    "    #     'Not B': [B_wo_A, wo_A_and_B]\n",
    "    # }, index=['A', 'Not A'])\n",
    "\n",
    "    contingency_table = pd.DataFrame({\n",
    "        \"mut_A\": [jaccard_index_dict[\"A_and_B\"], jaccard_index_dict[\"A_and_B_wt\"]], \n",
    "        \"wt_A\": [jaccard_index_dict[\"A_wt_and_B\"], jaccard_index_dict[\"A_wt_and_B_wt\"]]}, index=['mut_B', 'wt_B'])\n",
    "\n",
    "    print(\"Contingency Table for pair\", pair)\n",
    "    print(contingency_table)\n",
    "\n",
    "\n",
    "# Get final combination based on threshold\n",
    "jacc_ind_the = 0.7\n",
    "overlap_the = 0.7\n",
    "\n",
    "# Where non wt are max and above threshold\n",
    "mutation = {\"Type\" : [], \"Position\" : [], \"Variant\" : [], \"Frequency\" : []}\n",
    "\n",
    "for i in range(len(jaccard_index_meta[\"Pair\"])):\n",
    "    max_type = max(jaccard_index_meta[\"Jaccard Index Dict\"][i], key=jaccard_index_meta[\"Jaccard Index Dict\"][i].get)\n",
    "    if (max(jaccard_index_meta[\"Jaccard Index Dict\"][i].values()) > jacc_ind_the) and (max_type != \"A_wt_and_B_wt\"):\n",
    "        mutation[\"Type\"].append(max_type)\n",
    "        if max_type == \"A_and_B\":\n",
    "            mutation[\"Position\"] += [jaccard_index_meta[\"Pair\"][i][0], jaccard_index_meta[\"Pair\"][i][1]]\n",
    "            mutation[\"Variant\"] +=  [f'{ref_nuc_pair1}{jaccard_index_meta[\"Pair\"][i][0]}{max_nucleotide_pair1}', f'{ref_nuc_pair2}{jaccard_index_meta[\"Pair\"][i][1]}{max_nucleotide_pair2}']\n",
    "            mutation[\"Frequency\"] += [jaccard_index_meta[\"Jaccard Index Dict\"][i][\"A_and_B\"], jaccard_index_meta[\"Jaccard Index Dict\"][i][\"A_and_B\"]]\n",
    "        elif max_type == \"A_and_B_wt\":\n",
    "            mutation[\"Position\"].append(jaccard_index_meta[\"Pair\"][i][0])\n",
    "            mutation[\"Variant\"].append(f'{ref_nuc_pair1}{jaccard_index_meta[\"Pair\"][i][0]}{max_nucleotide_pair1}')\n",
    "            mutation[\"Frequency\"].append(jaccard_index_meta[\"Jaccard Index Dict\"][i][\"A_and_B_wt\"])\n",
    "        elif max_type == \"A_wt_and_B\":\n",
    "            mutation[\"Position\"].append(jaccard_index_meta[\"Pair\"][i][1])\n",
    "            mutation[\"Variant\"].append(f'{ref_nuc_pair1}{jaccard_index_meta[\"Pair\"][i][1]}{max_nucleotide_pair1}')\n",
    "            mutation[\"Frequency\"].append(jaccard_index_meta[\"Jaccard Index Dict\"][i][\"A_wt_and_B\"])\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Masking the lower triangle of the matrix\n",
    "mask = np.tri(pval_matrix.shape[0], k=-1)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "#sns.heatmap(odds_ratio_matrix, cmap='coolwarm', mask=mask, linewidths=0.5, linecolor='white')\n",
    "sns.heatmap(jaccard_index_matrix, cmap='coolwarm', mask=mask, linewidths=0.5, linecolor='white')\n",
    "sns.xticklabels = nb_positions\n",
    "sns.yticklabels = nb_positions\n",
    "#Label the axes\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Position')\n",
    "plt.title(\"Jacard Index\")\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
